<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <title>EG-Lowlight</title>
    <meta name="author" content="Guoqiang Liang">
    <meta name="description" content="Project page of Towards Robust Event-guided Low-Light Image Enhancement">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="icon" type="image/png" href="eccv_logo.png">
    <!-- Format -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="../format/app.css">
    <link rel="stylesheet" href="../format/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <script src="../format/app.js"></script>

  </head>

  <body style=“text-align: center;”>
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-12 text-center">
                Towards Robust Event-guided Low-Light Image Enhancement: A Large-Scale Real-World Event-Image Dataset and Novel Approach  <br />          
                <small>
                    Accepted to CVPR 2024
                </small>
            </h1>
        </div>
        
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
			
			<img src="./image/guoqiangliang.png" height="80px"><br>
                        Guoqiang Liang
                      </a>
                        <br /> AI Thrust, HKUST(GZ)
                      <br /> &nbsp &nbsp
                    </li>
                  
                    <li>

            <img src="./image/kanghao.jpg" height="80px"><br>
                        Kanghao Chen
                      </a>
                        <br /> AI Thrust, HKUST(GZ)
                      <br /> &nbsp &nbsp
                    </li>
                  
                    <li>

            <img src="./image/hangyu.jpg" height="80px"><br>
                        Hangyu Li
                      </a>
                        <br /> AI Thrust, HKUST(GZ)
                      <br /> &nbsp &nbsp
                    </li>
                  
                    <li>

            
            <img src="./image/yunfanlu.jpg" height="80px"><br>
                        Yunfan Lu
                       </a>
                       <br /> AI Thrust, HKUST(GZ)
                       <br /> &nbsp &nbsp
                   </li>

                   <li>

			<img src="./image/Addision.png " height="80px"><br>
                        <a href="https://addisonwang2013.github.io/vlislab/linwang.html">
                           Addison Lin Wang
                        </a>
                        <br /> AI & CMA Thrust, HKUST(GZ) 
			    <br/> Dept. of CSE, HKUST 
                    </li>
                </ul>
            </div>
        </div>



        <!-- ##### Elements #####-->
        <div class="row">
                <div class="col-md-8 col-md-offset-2 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
		                    <a href="">
                            <img src="image/arxiv.png" height="100px"><br>
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/EthanLiang99/EvLight">
                            <img src="image/github_icon.jpeg" height="100px"><br>
                                    <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://vlislab22.github.io/vlislab/">
                            <img src="image/lab_logo.png" height="100px"><br>
                                <h4><strong>Vlislab</strong></h4>
                            </a>
                        </li>             
                                           
                    </ul>
                </div>
        </div>


        <!-- ##### Abstract #####-->
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Event camera has recently received much attention for low-light image enhancement (LIE) thanks to their distinct advantages, such as high dynamic range.
                    However, current research is prohibitively restricted by the lack of large-scale, real-world, and spatial-temporally aligned event-image datasets.
                    To this end, we propose a real-world (indoor and outdoor) dataset comprising over 30K pairs of images and events under both low and normal illumination conditions. 
                    To achieve this, we utilize a robotic arm that traces a consistent non-linear trajectory to curate the dataset with spatial alignment precision under 0.03mm. 
                    We then introduce a matching alignment strategy, rendering 90% of our dataset with errors less than 0.01s.
                    Based on the dataset, we propose a novel event-guided LIE approach, called EvLight, towards robust performance in real-world low-light scenes.
                    Specifically, we first design the multi-scale holistic fusion branch to extract holistic structural and textural information from both events and images.
                    To ensure robustness against variations in the regional illumination and noise, we then introduce a Signal-to-Noise-Ratio (SNR)-guided regional feature selection to selectively fuse features of images from regions with high SNR and enhance those with low SNR by extracting regional structure information from events.
                    Extensive experiments on our dataset and the synthetic SDSD dataset demonstrate our EvLight significantly surpasses the frame-based methods, e.g., Retinexformer by 1.14 dB and 2.62 dB, respectively.
                </p>
            </div>
        </div>
        
 

        <!-- ##### Results #####-->

    <div class="row">     
        <div class="col-md-8 col-md-offset-2">
            <h3>
                Overall framework of our EvLight
            </h3>
        <p class="text-justify">
            An overview of our framework. Our method consists of three parts, (a) Preprocessing, (b) SNR-guided Regional Feature Selection, and (c) Holistic-Regional Fusion Branch. Specifically, SNR-guided Regional Feature Selection consists of two parts: Image-Regional Feature Selection (IRFS) and Event-Regional Feature Selection (ERFS). Additionally, Holistic-Regional Fusion Branch encompasses Holistic Feature Extraction (HFE) and Holistic-Regional Feature Fusion (HRF).
        </p>
            <img src="image/framework-3.png" class="img-responsive" alt="vis_res"  class="center" >
            </div>
    </div>

    <div class="row">     
        <div class="col-md-8 col-md-offset-2">
            <h3>
                Collection of our SDE dataset
            </h3>
            <p class="text-justify">
                (a) An illustration of collecting spatially-aligned image-event dataset by mounting a DAVIS 346 event camera on the robotic arm and recording the sequences with the same trajectory receptively. (b) An overview of our matching alignment strategy. (c) An example of our dataset with images and paired events captured in low-light (with an ND8 filter) and normal-light conditions.
            </p>
		<img src="image/align_image2.png" class="img-responsive" alt="vis_res"  class="center" >
      	</div>
    </div>
   
     <div class="row">     
       
     <div class="col-md-8 col-md-offset-2">
        </h3>
            Visual results on our SDE dataset and SDSD dataset
        </h3>
		    <video width="854" height="480" controls src="visual-result-wo-ID-lr.mov" class="img-responsive" alt="vis_res" class="center"><br>        
    </div>
    
    <div class="row">     
        <div class="col-md-8 col-md-offset-2">
        </h3>
                Example of the camera motion during capturing dataset
            </h3>
            <video width="854" height="480" controls src="SDE-Dataset-Trajectory-example-wo-ID.mov" class="img-responsive" alt="vis_res" class="center"><br>
      	</div>
    </div>

    <div class="row">     
        <div class="col-md-8 col-md-offset-2">
        </h3>
                Example of our SDE dataset (indoor + ourdoor)
            </h3>
            <video width="854" height="480" controls src="SDE-Dataset-Example-wo-ID.mov" class="img-responsive" alt="vis_res" class="center"><br>
      	</div>
    </div>
    

      </div>

        <!-- ##### Approach #####-->
                    
    </div>
   <!-- ##### BibTex #####-->

    </div>
</body>
</html>
